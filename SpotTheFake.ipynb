{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "authorship_tag": "ABX9TyPDEA6zBEKrRyVwjEyLXwTC"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Spot the Fake - CipherCop"
      ],
      "metadata": {
        "id": "E7JEOkZpIPmx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load and process data"
      ],
      "metadata": {
        "id": "v7C-Bh_XIWzX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kc5dxph61b0c",
        "outputId": "e0207ffe-4b1a-44d7-acf8-13585e47a0a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "\n",
        "def clean_and_verify_paths(csv_path, images_folder):\n",
        "    \"\"\"\n",
        "    Reads a CSV, cleans filename entries, and verifies they exist.\n",
        "    Returns a dictionary mapping brands to valid image paths.\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(csv_path)\n",
        "    brand_to_images = defaultdict(list)\n",
        "    found_count = 0\n",
        "    not_found_count = 0\n",
        "\n",
        "    # Get a set of all actual filenames for fast lookup\n",
        "    # os.listdir() is efficient for getting all filenames in a folder\n",
        "    all_actual_files = set(os.listdir(images_folder))\n",
        "\n",
        "    for index, row in df.iterrows():\n",
        "        # Clean the filename from the CSV\n",
        "        original_filename = str(row['fileName']).strip()  # Remove whitespace\n",
        "        brand_name = row['logoName']\n",
        "\n",
        "        # Normalize the filename to lowercase for a case-insensitive check\n",
        "        normalized_filename = original_filename.lower()\n",
        "\n",
        "        # Check if the cleaned file exists using a case-insensitive match\n",
        "        matched_file = None\n",
        "        for actual_file in all_actual_files:\n",
        "            if actual_file.lower() == normalized_filename:\n",
        "                matched_file = actual_file\n",
        "                break\n",
        "\n",
        "        if matched_file:\n",
        "            # Construct the full path and add to the dictionary\n",
        "            full_path = os.path.join(images_folder, matched_file)\n",
        "            brand_to_images[brand_name].append(full_path)\n",
        "            found_count += 1\n",
        "        else:\n",
        "            not_found_count += 1\n",
        "            # You can add a print statement or log here for debugging\n",
        "            # print(f\"Warning: File not found for '{original_filename}'\")\n",
        "\n",
        "    print(f\"Verified {found_count} files, skipped {not_found_count} not found.\")\n",
        "    return brand_to_images\n"
      ],
      "metadata": {
        "id": "iE6Y4yUrrKf0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logos_folder_name = '/content/drive/My Drive/AMPBA/CipherCop/Logos/'\n",
        "csv_file_path = '/content/drive/My Drive/AMPBA/CipherCop/LogoDatabase.csv'\n",
        "cleaned_brand_dict = clean_and_verify_paths(csv_file_path, logos_folder_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VxXINH_VstkK",
        "outputId": "d9deed58-e04a-4bd7-a6da-62b9eb22eb77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Verified 1447 files, skipped 34 not found.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Process data to create pairs for Siamese network"
      ],
      "metadata": {
        "id": "9HF6HykYIdEp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "def create_siamese_pairs(brand_dict, num_pairs = 500):\n",
        "  '''\n",
        "  Set up the data by creating pairs of images. The number of similar pairs\n",
        "  and the number of dissimilar pairs would be equal to num_pairs.\n",
        "  So a total of 2 * num_pairs pairs would be created.\n",
        "  Label will be set to 1 for similar pairs and 0 for dissimilar pairs.\n",
        "  '''\n",
        "  all_pairs = []\n",
        "\n",
        "  # Similar pairs\n",
        "  brands_with_multiple_images = {brand: images for brand, images in brand_dict.items() if len(images) > 1}\n",
        "  brand_list = list(brands_with_multiple_images.keys())\n",
        "  for _ in range(num_pairs):\n",
        "    brand_name = random.choice(brand_list)\n",
        "    images = brands_with_multiple_images[brand_name]\n",
        "    img1_path, img2_path = random.sample(images, 2)\n",
        "    all_pairs.append({'image1_path': img1_path, 'image2_path': img2_path, 'label': 1})\n",
        "\n",
        "  # Dissimilar pairs\n",
        "  brand_list = list(brand_dict.keys())\n",
        "  for _ in range(num_pairs):\n",
        "    brand1_name, brand2_name = random.sample(brand_list, 2)\n",
        "    img1_path = random.choice(brand_dict[brand1_name])\n",
        "    img2_path = random.choice(brand_dict[brand2_name])\n",
        "    all_pairs.append({'image1_path': img1_path, 'image2_path': img2_path, 'label': 0})\n",
        "\n",
        "    return all_pairs"
      ],
      "metadata": {
        "id": "pXLix38-5FsQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get Siamese pairs and labels\n",
        "all_pairs = create_siamese_pairs(cleaned_brand_dict, num_pairs = 500)"
      ],
      "metadata": {
        "id": "KvLhFsFc-GPY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build Siamese network model"
      ],
      "metadata": {
        "id": "Td2vdGGCIqLT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create class for Siamese Network\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "class SiameseNetwork(nn.Module):\n",
        "  def __init__(self, base_model):\n",
        "    super(SiameseNetwork, self).__init__()\n",
        "    self.base_model = base_model\n",
        "    # Remove the last classification layer\n",
        "    # This works for ResNet, not for VGG\n",
        "    if hasattr(self.base_model, 'fc'):\n",
        "      self.base_model.fc = nn.Identity()\n",
        "\n",
        "  def forward(self, input1, input2):\n",
        "    output1 = self.base_model(input1)\n",
        "    output2 = self.base_model(input2)\n",
        "    return output1, output2  # Returns feature embeddings\n"
      ],
      "metadata": {
        "id": "_Gck86Qf_Lut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose a pre-trained model resnet18\n",
        "# Experiment with more sophisticated models like resnet50 if time permits\n",
        "resnet18 = models.resnet18(pretrained=True)\n",
        "\n",
        "# Create the Siamese network\n",
        "siamese_net_model = SiameseNetwork(resnet18)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XLOWStM4BmFD",
        "outputId": "a9460f0c-af0b-4e1c-eca6-0b80a6d39924"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 231MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Apply image transformations"
      ],
      "metadata": {
        "id": "UB--ginRIwJN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform images for loading into the model\n",
        "class SiameseDataset(Dataset):\n",
        "  def __init__(self, all_pairs, transform=None):\n",
        "    self.all_pairs  = all_pairs\n",
        "    self.transform = transform\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.all_pairs)\n",
        "\n",
        "  def __getitem__(self, idx): # Corrected method name\n",
        "    pair_dict = self.all_pairs[idx]\n",
        "\n",
        "    # Unpack from dictionary\n",
        "    img1_path = pair_dict['image1_path']\n",
        "    img2_path = pair_dict['image2_path']\n",
        "    label = pair_dict['label']\n",
        "\n",
        "    img1 = Image.open(img1_path).convert('RGB')\n",
        "    img2 = Image.open(img2_path).convert('RGB')\n",
        "\n",
        "    if self.transform:\n",
        "      img1 = self.transform(img1)\n",
        "      img2 = self.transform(img2)\n",
        "\n",
        "    return img1, img2, torch.tensor(label, dtype=torch.float32)"
      ],
      "metadata": {
        "id": "ON-CpgJ2EMjD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define transformations\n",
        "data_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)), # Resize images to a fixed size\n",
        "    transforms.ToTensor(),         # Convert PIL Image to PyTorch tensor\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # Normalize with ImageNet stats\n",
        "])\n"
      ],
      "metadata": {
        "id": "GYBTaM6VFfrW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set up data for loading into model"
      ],
      "metadata": {
        "id": "v8p1nofHI2Y9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the dataset instance\n",
        "siamese_dataset = SiameseDataset(all_pairs, transform=data_transforms)\n",
        "\n",
        "# Create the DataLoader instance\n",
        "batch_size = 32\n",
        "siamese_dataloader = DataLoader(siamese_dataset, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "yPl4wTmeFkjS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set up contrastive loss function, Optimizer"
      ],
      "metadata": {
        "id": "gEV93ll4I7rR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define contrastive loss function\n",
        "class ContrastiveLoss(nn.Module):\n",
        "  '''\n",
        "  Based on: http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n",
        "  '''\n",
        "  def __init__(self, margin=2.0):\n",
        "    super(ContrastiveLoss, self).__init__()\n",
        "    self.margin = margin\n",
        "\n",
        "  def forward(self, output1, output2, label):\n",
        "    euclidean_distance = F.pairwise_distance(output1, output2)\n",
        "\n",
        "    loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) + \\\n",
        "      (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))\n",
        "    return loss_contrastive"
      ],
      "metadata": {
        "id": "ZSskyVCQFybn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the loss function\n",
        "contrastive_loss = ContrastiveLoss(margin=2.0)"
      ],
      "metadata": {
        "id": "z4-7WN_YGxet"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up optimizer\n",
        "optimizer = optim.Adam(siamese_net_model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "bK1Y77cuG6Bz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set up device and train model"
      ],
      "metadata": {
        "id": "47z67TXIJDne"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up the device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "siamese_net_model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UdDALJGtHItp",
        "outputId": "0ee17900-872a-4083-c8d7-4a0331ae660f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SiameseNetwork(\n",
              "  (base_model): ResNet(\n",
              "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu): ReLU(inplace=True)\n",
              "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "    (layer1): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (layer2): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (layer3): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (layer4): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "    (fc): Identity()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up the training loop\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "  siamese_net_model.train()\n",
        "  running_loss = 0.0\n",
        "\n",
        "  # Iterate over the images in data\n",
        "  for img1_batch, img2_batch, label_batch in siamese_dataloader:\n",
        "    img1_batch = img1_batch.to(device)\n",
        "    img2_batch = img2_batch.to(device)\n",
        "    label_batch = label_batch.to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    output1, output2 = siamese_net_model(img1_batch, img2_batch)\n",
        "\n",
        "    loss = contrastive_loss(output1, output2, label_batch)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    running_loss += loss.item()\n",
        "\n",
        "  epoch_loss = running_loss / len(siamese_dataloader)\n",
        "  print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P6eJ9KjnHWnO",
        "outputId": "f6e79871-146f-4289-ff42-7f3aa4007079"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 0.8692\n",
            "Epoch 2/10, Loss: 0.3970\n",
            "Epoch 3/10, Loss: 0.0971\n",
            "Epoch 4/10, Loss: 0.0232\n",
            "Epoch 5/10, Loss: 0.0627\n",
            "Epoch 6/10, Loss: 0.1223\n",
            "Epoch 7/10, Loss: 0.0642\n",
            "Epoch 8/10, Loss: 1.9810\n",
            "Epoch 9/10, Loss: 0.1248\n",
            "Epoch 10/10, Loss: 0.1138\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model\n",
        "torch.save(siamese_net_model.state_dict(), 'siamese_net_model.pth')\n",
        "\n",
        "# Model full\n",
        "#torch.save('siamese_net_model_full.pth')"
      ],
      "metadata": {
        "id": "wGx_0bFVz-eE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implement Explainable AI"
      ],
      "metadata": {
        "id": "46KVL8j-JIbI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Grad-CAM"
      ],
      "metadata": {
        "id": "qQJ4W4KOJQp5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "siamese_net_model.eval()\n",
        "\n",
        "# Choose an image pair for demonstration (using the same pair as Saliency Maps)\n",
        "img1, img2, label = siamese_dataset[0]\n",
        "\n",
        "# Move the images to the device and add a batch dimension\n",
        "img1 = img1.to(device).unsqueeze(0)\n",
        "img2 = img2.to(device).unsqueeze(0)\n",
        "\n",
        "# We need to access the feature maps from a convolutional layer\n",
        "# For ResNet18, a good layer to target is the last convolutional block before the fully connected layer\n",
        "target_layer = siamese_net_model.base_model.layer4[-1]\n",
        "\n",
        "# Clear any existing hooks on the target layer\n",
        "# This is important if the cell has been run multiple times\n",
        "if hasattr(target_layer, '_forward_hooks'):\n",
        "    target_layer._forward_hooks.clear()\n",
        "if hasattr(target_layer, '_backward_hooks'):\n",
        "    target_layer._backward_hooks.clear()\n",
        "if hasattr(target_layer, '_full_backward_hooks'):\n",
        "    target_layer._full_backward_hooks.clear()\n",
        "\n",
        "\n",
        "# Store gradients and activations\n",
        "gradients = None\n",
        "activations = None\n",
        "\n",
        "def save_gradients(module, grad_input, grad_output):\n",
        "    global gradients\n",
        "    # We are interested in the gradient of the output of the layer\n",
        "    gradients = grad_output[0]\n",
        "\n",
        "def save_activations(module, input, output):\n",
        "    global activations\n",
        "    activations = output\n",
        "\n",
        "# Register hooks to the target layer\n",
        "hook_grad = target_layer.register_full_backward_hook(save_gradients)\n",
        "hook_act = target_layer.register_forward_hook(save_activations)\n",
        "\n",
        "# Forward pass through the model\n",
        "output1, output2 = siamese_net_model(img1, img2)\n",
        "\n",
        "# Calculate the distance between the embeddings\n",
        "euclidean_distance = torch.nn.functional.pairwise_distance(output1, output2)\n",
        "\n",
        "# Backpropagate the gradient of the distance with respect to the output\n",
        "# We'll backpropagate a scalar '1' to get the gradients of the distance\n",
        "euclidean_distance.backward(torch.ones_like(euclidean_distance))\n",
        "\n",
        "# Remove hooks\n",
        "hook_grad.remove()\n",
        "hook_act.remove()\n",
        "\n",
        "# Get the weights for Grad-CAM by global average pooling the gradients\n",
        "# Gradients have shape (batch_size, channels, height, width)\n",
        "# Average across height and width dimensions\n",
        "pooled_gradients = torch.mean(gradients, dim=[2, 3])\n",
        "\n",
        "# Weight the channels of the activation maps by the pooled gradients\n",
        "# Activations have shape (batch_size, channels, height, width)\n",
        "for i in range(activations.shape[0]):\n",
        "    for j in range(pooled_gradients.shape[1]):\n",
        "        activations[i, j, :, :] *= pooled_gradients[i, j]\n",
        "\n",
        "# Sum the weighted activation maps across channels to get the heatmap\n",
        "heatmap = torch.sum(activations, dim=1).squeeze(0) # Remove batch and channel dimensions\n",
        "\n",
        "# Apply ReLU to the heatmap\n",
        "heatmap = F.relu(heatmap)\n",
        "\n",
        "# Normalize the heatmap\n",
        "# Add a small epsilon to avoid division by zero if max is 0\n",
        "heatmap = heatmap / (torch.max(heatmap) + 1e-8)\n",
        "\n",
        "# Upsample the heatmap to the original image size\n",
        "# Need to reverse the transforms for visualization\n",
        "# We can resize the heatmap to the size of the input image before transforms (224, 224)\n",
        "heatmap = F.interpolate(heatmap.unsqueeze(0).unsqueeze(0), size=(224, 224), mode='bilinear', align_corners=False)\n",
        "heatmap = heatmap.squeeze().detach().cpu().numpy()\n",
        "\n",
        "# Convert the original image tensor back to PIL Image for visualization\n",
        "img1_display = img1.squeeze(0).detach().cpu()\n",
        "mean = torch.tensor([0.485, 0.456, 0.406]).view(-1, 1, 1)\n",
        "std = torch.tensor([0.229, 0.224, 0.225]).view(-1, 1, 1)\n",
        "img1_display = img1_display * std + mean\n",
        "img1_display = torch.clamp(img1_display, 0, 1)\n",
        "img1_display = transforms.ToPILImage()(img1_display)\n",
        "\n",
        "\n",
        "# Visualize the original image and the Grad-CAM heatmap\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(img1_display)\n",
        "plt.title(\"Original Image\")\n",
        "plt.axis('off')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(img1_display) # Display original image\n",
        "plt.imshow(heatmap, cmap='jet', alpha=0.5) # Overlay heatmap\n",
        "plt.title(\"Grad-CAM Heatmap\")\n",
        "plt.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Grad-CAM heatmap generated for the first image of the pair.\")"
      ],
      "metadata": {
        "id": "2A7dNxrOJSxb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Saliency maps"
      ],
      "metadata": {
        "id": "2-5rssLZJOcA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the model to evaluation mode\n",
        "siamese_net_model.eval()\n",
        "\n",
        "# Choose an image pair from the dataset for demonstration\n",
        "# We'll use the first pair from the optimized dataset\n",
        "img1, img2, label = siamese_dataset[0]\n",
        "\n",
        "# Move the images to the device\n",
        "img1 = img1.to(device)\n",
        "img2 = img2.to(device)\n",
        "\n",
        "# We need to calculate gradients with respect to the input image, so we set requires_grad to True\n",
        "img1.requires_grad_(True)\n",
        "img2.requires_grad_(True)\n",
        "\n",
        "\n",
        "# Forward pass through the model\n",
        "output1, output2 = siamese_net_model(img1.unsqueeze(0), img2.unsqueeze(0)) # Add batch dimension\n",
        "\n",
        "# Calculate the distance between the embeddings\n",
        "euclidean_distance = torch.nn.functional.pairwise_distance(output1, output2)\n",
        "\n",
        "# To get the saliency map for img1 with respect to the distance,\n",
        "# we need to backpropagate the gradient of the distance to img1.\n",
        "# We can treat the distance as a scalar output for this purpose.\n",
        "euclidean_distance.backward()\n",
        "\n",
        "# Get the gradient with respect to img1\n",
        "saliency_map = img1.grad.data.abs().mean(dim=0) # Take the mean across color channels\n",
        "\n",
        "# Normalize the saliency map for better visualization\n",
        "saliency_map = saliency_map / saliency_map.max()\n",
        "\n",
        "# Convert the image tensor back to PIL Image for visualization\n",
        "# We need to reverse the normalization and permutations\n",
        "img1_display = img1.detach().cpu()\n",
        "# Reverse normalization (using mean and std used in transforms)\n",
        "mean = torch.tensor([0.485, 0.456, 0.406]).view(-1, 1, 1)\n",
        "std = torch.tensor([0.229, 0.224, 0.225]).view(-1, 1, 1)\n",
        "img1_display = img1_display * std + mean\n",
        "img1_display = torch.clamp(img1_display, 0, 1) # Clamp values to be within [0, 1]\n",
        "img1_display = transforms.ToPILImage()(img1_display)\n",
        "\n",
        "\n",
        "# Visualize the original image and the saliency map\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(img1_display)\n",
        "plt.title(\"Original Image\")\n",
        "plt.axis('off')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(saliency_map.cpu(), cmap='hot')\n",
        "plt.title(\"Saliency Map\")\n",
        "plt.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Saliency Map generated for the first image of the pair.\")"
      ],
      "metadata": {
        "id": "vFiPmcC1JN7-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}